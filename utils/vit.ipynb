{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 17:18:11.203676: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-29 17:18:11.233086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 17:18:11.758814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch.distributions import Multinomial, Bernoulli\n",
    "from tensorboard_logger import configure, log_value\n",
    "from utils import utils\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b_16 = torchvision.models.vit_b_16(weights='ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32,scale=True),\n",
    "    v2.Resize(size=384,interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    v2.CenterCrop(384),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32,scale=True),\n",
    "    v2.Resize(size=384,interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    v2.CenterCrop(384),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = torchvision.datasets.ImageFolder(root=\"/media/samar/HDD1T/rl/PatchDrop-main/200-dataset/train\",transform=train_transform)\n",
    "valdata = torchvision.datasets.ImageFolder(root=\"/media/samar/HDD1T/rl/PatchDrop-main/200-dataset/val\",transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(traindata,batch_size=8,num_workers = 8,shuffle=True)\n",
    "valloader = DataLoader(valdata,batch_size=8,num_workers = 8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = utils.get_model()\n",
    "agent.cuda()\n",
    "vit_b_16.eval().cuda() # HR Classifier is Fixed\n",
    "mappings, _, patch_size = utils.action_space_model()\n",
    "start_epoch = 0\n",
    "ppo_epochs = 3\n",
    "clip = 0.2\n",
    "test_interval = 1\n",
    "lr_size = 96\n",
    "alpha = 0.8\n",
    "penalty = -0.5\n",
    "max_epochs = 10\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "cv_dir = \"/media/samar/HDD1T/rl/PatchDrop-main/cv_dir/pretrain\"\n",
    "configure(cv_dir+'/log', flush_secs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # This steps trains the policy network only\n",
    "    agent.train()\n",
    "    matches, rewards, rewards_baseline, policies = [], [], [], []\n",
    "    for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        inputs, targets = Variable(inputs), Variable(targets).cuda(non_blocking=True)\n",
    "        \n",
    "        inputs = inputs.cuda()\n",
    "        inputs_agent = inputs.clone()\n",
    "        inputs_map = inputs.clone()\n",
    "        inputs_sample = inputs.clone()\n",
    "\n",
    "        # Run the low-res image through Policy Network\n",
    "        inputs_agent = torch.nn.functional.interpolate(inputs_agent, (lr_size, lr_size))\n",
    "        probs = F.sigmoid(agent.forward(inputs_agent))\n",
    "        probs = probs*alpha + (1-alpha) * (1-probs)\n",
    "\n",
    "        # Sample the policies from the Bernoulli distribution characterized by agent's output\n",
    "        distr = Bernoulli(probs)\n",
    "        policy_sample = distr.sample()\n",
    "\n",
    "        # Test time policy - used as baseline policy in the training step\n",
    "        policy_map = probs.data.clone()\n",
    "        policy_map[policy_map<0.5] = 0.0\n",
    "        policy_map[policy_map>=0.5] = 1.0\n",
    "        # Agent sampled high resolution images\n",
    "        inputs_map = utils.agent_chosen_input(inputs_map, policy_map, mappings, patch_size)\n",
    "        inputs_sample = utils.agent_chosen_input(inputs_sample, policy_sample.int(), mappings, patch_size)\n",
    "\n",
    "        # Forward propagate images through the classifiers\n",
    "        preds_map = vit_b_16(inputs_map)\n",
    "        preds_sample = vit_b_16(inputs_sample)\n",
    "\n",
    "        # Find the reward for baseline and sampled policy\n",
    "        reward_map, match = utils.compute_reward(preds_map, targets, policy_map.data, penalty)\n",
    "        reward_sample, _ = utils.compute_reward(preds_sample, targets, policy_sample.data, penalty)\n",
    "        advantage = reward_sample.cuda().float() - reward_map.cuda().float()\n",
    "\n",
    "        # Find the loss for only the policy network\n",
    "        loss = -distr.log_prob(policy_sample)\n",
    "        loss = loss * Variable(advantage).expand_as(policy_sample)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        matches.append(match.cpu())\n",
    "        rewards.append(reward_sample.cpu())\n",
    "        rewards_baseline.append(reward_map.cpu())\n",
    "        policies.append(policy_sample.data.cpu())\n",
    "\n",
    "    accuracy, reward, sparsity, variance, policy_set = utils.performance_stats(policies, rewards, matches)\n",
    "\n",
    "    print('Train: %d | Acc: %.3f | Rw: %.2E | S: %.3f | V: %.3f | #: %d'%(epoch, accuracy, reward, sparsity, variance, len(policy_set)))\n",
    "    log_value('train_accuracy', accuracy, epoch)\n",
    "    log_value('train_reward', reward, epoch)\n",
    "    log_value('train_sparsity', sparsity, epoch)\n",
    "    log_value('train_variance', variance, epoch)\n",
    "    log_value('train_baseline_reward', torch.cat(rewards_baseline, 0).mean(), epoch)\n",
    "    log_value('train_unique_policies', len(policy_set), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    matches, rewards, policies = [], [], []\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(valloader), total=len(valloader)):\n",
    "\n",
    "          inputs, targets = Variable(inputs), Variable(targets).cuda(non_blocking=True)\n",
    "          \n",
    "          inputs = inputs.cuda()\n",
    "\n",
    "          # Get the low resolution agent images\n",
    "          inputs_agent = inputs.clone()\n",
    "          inputs_agent = torch.nn.functional.interpolate(inputs_agent, (lr_size, lr_size))\n",
    "          probs = F.sigmoid(agent.forward(inputs_agent))\n",
    "\n",
    "          # Sample the test-time policy\n",
    "          policy = probs.data.clone()\n",
    "          policy[policy<0.5] = 0.0\n",
    "          policy[policy>=0.5] = 1.0\n",
    "\n",
    "          # Get the masked high-res image and perform inference\n",
    "          inputs = utils.agent_chosen_input(inputs, policy, mappings, patch_size)\n",
    "          preds = vit_b_16(inputs)\n",
    "\n",
    "          reward, match = utils.compute_reward(preds, targets, policy.data, penalty)\n",
    "\n",
    "          matches.append(match)\n",
    "          rewards.append(reward)\n",
    "          policies.append(policy.data)\n",
    "\n",
    "    accuracy, reward, sparsity, variance, policy_set = utils.performance_stats(policies, rewards, matches)\n",
    "\n",
    "    print('Test - Acc: %.3f | Rw: %.2E | S: %.3f | V: %.3f | #: %d'%(accuracy, reward, sparsity, variance, len(policy_set)))\n",
    "    log_value('test_accuracy', accuracy, epoch)\n",
    "    log_value('test_reward', reward, epoch)\n",
    "    log_value('test_sparsity', sparsity, epoch)\n",
    "    log_value('test_variance', variance, epoch)\n",
    "    log_value('test_unique_policies', len(policy_set), epoch)\n",
    "\n",
    "    # Save the Policy Network - Classifier is fixed in this phase\n",
    "    agent_state_dict = agent.state_dict()\n",
    "    state = {\n",
    "      'agent': agent_state_dict,\n",
    "      'epoch': epoch,\n",
    "      'reward': reward,\n",
    "      'acc': accuracy\n",
    "    }\n",
    "    torch.save(state, cv_dir+'/ckpt_E_%d_A_%.3f_R_%.2E'%(epoch, accuracy, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:49:16<00:00,  2.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0 | Acc: 0.768 | Rw: 4.90E-01 | S: 6.818 | V: 1.677 | #: 24568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6250 [00:00<?, ?it/s]/tmp/ipykernel_2532790/3704413837.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets).cuda(non_blocking=True)\n",
      "100%|██████████| 6250/6250 [22:09<00:00,  4.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Acc: 0.781 | Rw: 5.66E-01 | S: 5.862 | V: 0.345 | #: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:56:58<00:00,  2.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1 | Acc: 0.776 | Rw: 4.94E-01 | S: 6.876 | V: 1.642 | #: 19522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [21:00<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Acc: 0.783 | Rw: 5.63E-01 | S: 6.026 | V: 0.451 | #: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:56:22<00:00,  2.36it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 | Acc: 0.781 | Rw: 4.92E-01 | S: 6.978 | V: 1.631 | #: 19800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [21:16<00:00,  4.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Acc: 0.784 | Rw: 5.64E-01 | S: 6.055 | V: 0.266 | #: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:58:18<00:00,  2.34it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3 | Acc: 0.773 | Rw: 4.92E-01 | S: 6.798 | V: 1.606 | #: 17861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [21:10<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Acc: 0.785 | Rw: 5.67E-01 | S: 6.000 | V: 0.010 | #: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [2:58:46<00:00,  2.33it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4 | Acc: 0.774 | Rw: 4.93E-01 | S: 6.794 | V: 1.600 | #: 17618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [21:15<00:00,  4.90it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Acc: 0.785 | Rw: 5.67E-01 | S: 5.991 | V: 0.094 | #: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, start_epoch\u001b[38;5;241m+\u001b[39mmax_epochs):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m test_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         test(epoch)\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m advantage \u001b[38;5;241m=\u001b[39m reward_sample\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m-\u001b[39m reward_map\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Find the loss for only the policy network\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mdistr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m Variable(advantage)\u001b[38;5;241m.\u001b[39mexpand_as(policy_sample)\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.11/site-packages/torch/distributions/bernoulli.py:110\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m--> 110\u001b[0m logits, value \u001b[38;5;241m=\u001b[39m broadcast_all(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m, value)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbinary_cross_entropy_with_logits(logits, value, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/grad/lib/python3.11/site-packages/torch/distributions/utils.py:122\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped \u001b[38;5;241m=\u001b[39m wrapped\n\u001b[1;32m    120\u001b[0m     update_wrapper(\u001b[38;5;28mself\u001b[39m, wrapped)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch+max_epochs):\n",
    "    train(epoch)\n",
    "    if epoch % test_interval == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
